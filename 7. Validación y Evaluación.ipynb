{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70ecd96-a89f-47cd-89d0-1042906f2228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. **Validación y Evaluación**\n",
    "   - **Utilidad:** Este grupo de métodos se utiliza para evaluar el rendimiento del modelo y ajustar los hiperparámetros.\n",
    "   - **Funciones comunes:**\n",
    "     - **train_test_split:** Dividir los datos en conjuntos de entrenamiento y prueba.\n",
    "     - **cross_val_score:** Validación cruzada para estimar el rendimiento del modelo.\n",
    "     - **GridSearchCV:** Búsqueda en cuadrícula para la optimización de hiperparámetros.\n",
    "     - **RandomizedSearchCV:** Búsqueda aleatoria para la optimización de hiperparámetros.\n",
    "     - **accuracy_score:** Calcular la precisión del modelo.\n",
    "     - **confusion_matrix:** Generar la matriz de confusión.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288820f1-3d58-4f0f-935e-ce1be80fa584",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train_test_split\n",
    "Dividimos los datos en conjuntos de entrenamiento y prueba (80% y 20% respectivamente) y mostramos el tamaño de cada conjunto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4be5fd0c-9fd5-4d62-a49b-df57bbebc2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 120 samples\n",
      "Test set size: 30 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Training set size: {X_train.shape[0]} samples')\n",
    "print(f'Test set size: {X_test.shape[0]} samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc066e-1073-422d-851e-8cdabf172dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cross_val_score\n",
    "Realizamos una validación cruzada de 5 pliegues y mostramos las puntuaciones de cada pliegue y la media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5de8ee5-f96f-43f0-9654-092517fee5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.96666667 1.         0.93333333 0.96666667 1.        ]\n",
      "Average cross-validation score: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "\n",
    "print(f'Cross-validation scores: {scores}')\n",
    "print(f'Average cross-validation score: {scores.mean():.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee541e4-a9d7-40d6-b17d-e6073b88d9dd",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    "Realizamos una búsqueda en cuadrícula para encontrar los mejores hiperparámetros y mostramos los mejores parámetros y la mejor puntuación de validación cruzada.\n",
    "- Cargamos el dataset Wine y dividimos los datos en conjuntos de entrenamiento y prueba para clasificación.\n",
    "- Creamos un pipeline que primero escala los datos y luego aplica la regresión logística.\n",
    "- Definimos un param_grid que incluye los parámetros C, solver (solo liblinear para evitar problemas de convergencia) y max_iter para la búsqueda de hiperparámetros.\n",
    "- Inicializamos y ajustamos GridSearchCV para encontrar los mejores parámetros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de83a3da-7cf6-470a-b3a2-8d45392ee5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'logisticregression__C': 1, 'logisticregression__max_iter': 500, 'logisticregression__solver': 'liblinear'}\n",
      "Best cross-validation score: 0.99\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the dataset\n",
    "data = load_wine()\n",
    "X_clf = data.data\n",
    "y_clf = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'logisticregression__C': [0.1, 1, 10, 100],\n",
    "    'logisticregression__solver': ['liblinear'],\n",
    "    'logisticregression__max_iter': [500, 1000, 2000]\n",
    "}\n",
    "\n",
    "# Create a pipeline with scaling and logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logisticregression', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit GridSearchCV\n",
    "grid_search.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Best cross-validation score: {grid_search.best_score_:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc3da59-9f8f-40ba-97b4-7fbcf64c689e",
   "metadata": {},
   "source": [
    "## RandomizedSearchCV\n",
    "Búsqueda aleatoria para la optimización de hiperparámetros.\n",
    "- Cargamos el dataset Wine y dividimos los datos en conjuntos de entrenamiento y prueba para clasificación.\n",
    "- Creamos un pipeline que primero escala los datos y luego aplica la regresión logística.\n",
    "- Definimos un param_dist que incluye los parámetros C, solver (solo liblinear para evitar problemas de convergencia) y max_iter para la búsqueda de hiperparámetros.\n",
    "- Inicializamos y ajustamos RandomizedSearchCV para encontrar los mejores parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5a6b84e-827b-4625-83e8-d06797559c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'logisticregression__C': np.float64(2.1584494295802448), 'logisticregression__max_iter': 1000, 'logisticregression__solver': 'liblinear'}\n",
      "Best cross-validation score: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# Load the dataset\n",
    "data = load_wine()\n",
    "X_clf = data.data\n",
    "y_clf = data.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(X_clf, y_clf, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter distribution\n",
    "param_dist = {\n",
    "    'logisticregression__C': uniform(0.1, 100),\n",
    "    'logisticregression__solver': ['liblinear'],\n",
    "    'logisticregression__max_iter': [500, 1000, 2000]\n",
    "}\n",
    "\n",
    "# Create a pipeline with scaling and logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logisticregression', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(pipeline, param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "random_search.fit(X_train_clf, y_train_clf)\n",
    "\n",
    "print(f'Best parameters: {random_search.best_params_}')\n",
    "print(f'Best cross-validation score: {random_search.best_score_:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0689917d-69c9-4882-a5c9-0c9a3616f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ea394d1-c062-46c5-8539-0b766c046c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b6c71-625b-4c54-a098-c409054b557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a082029-3f5a-44aa-b18d-03b521bb0ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70c188d-5932-4670-babb-83bd9eb868dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
